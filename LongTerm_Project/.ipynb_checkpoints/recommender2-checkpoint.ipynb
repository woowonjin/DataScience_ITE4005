{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_file = sys.argv[1]\n",
    "# test_file = sys.argv[2]\n",
    "train_file = \"u5.base\"\n",
    "test_file = \"u5.test\"\n",
    "\n",
    "k = 50  # number of iteration\n",
    "alpha = 0.25  # learning rate\n",
    "_lambda = 0.1  # parameter for regularizer\n",
    "patience = 5  # number of patience that not improved from previous version\n",
    "attr_num = 12  # number of columns in factorized matrix\n",
    "column_names = [\"user_id\", \"item_id\", \"rating\", \"time_stamp\"]\n",
    "\n",
    "dataframe = pd.read_csv(train_file, sep=\"\\t\", names=column_names, header=None)\n",
    "testframe = pd.read_csv(test_file, sep=\"\\t\", names=column_names, header=None)\n",
    "\n",
    "dataframe = dataframe.drop([\"time_stamp\"], axis=1)\n",
    "testframe = testframe.drop([\"time_stamp\"], axis=1)\n",
    "\n",
    "train_movie_last = dataframe[\"item_id\"].max()\n",
    "test_movie_last = testframe[\"item_id\"].max()\n",
    "last_movie = max(train_movie_last, test_movie_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  item_id  rating\n",
      "6793        69     1134       5\n",
      "12405      130       82       5\n",
      "41047      404      323       3\n",
      "11533      118      675       5\n",
      "31975      320      403       4\n",
      "...        ...      ...     ...\n",
      "63206      650      218       3\n",
      "61404      627      808       2\n",
      "17730      197       39       2\n",
      "28030      293      770       3\n",
      "15725      178      483       4\n",
      "\n",
      "[76000 rows x 3 columns]\n",
      "       user_id  item_id  rating\n",
      "23438      256     1057       2\n",
      "78749      903     1073       3\n",
      "43994      427      990       5\n",
      "25959      277      844       4\n",
      "20526      224      470       4\n",
      "...        ...      ...     ...\n",
      "15858      180       79       3\n",
      "36880      368      219       2\n",
      "7921        85       53       3\n",
      "71524      772      259       2\n",
      "44208      429      366       3\n",
      "\n",
      "[4000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(dataframe, test_size=0.05, random_state=123)\n",
    "print(train_data)\n",
    "print(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id  1     2     3     4     5     6     7     8     9     10    ...  \\\n",
      "user_id                                                              ...   \n",
      "1         5.0   3.0   NaN   3.0   3.0   5.0   4.0   1.0   5.0   NaN  ...   \n",
      "2         4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   2.0  ...   \n",
      "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "5         4.0   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "939       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "940       NaN   NaN   NaN   2.0   NaN   NaN   NaN   5.0   3.0   NaN  ...   \n",
      "941       NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...   \n",
      "942       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "943       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "\n",
      "item_id  1673  1674  1675  1676  1677  1678  1679  1680  1681  1682  \n",
      "user_id                                                              \n",
      "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "5         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "939       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "940       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "941       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "942       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "943       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[943 rows x 1682 columns]\n"
     ]
    }
   ],
   "source": [
    "df_table = train_data.pivot(\"user_id\", \"item_id\", \"rating\")\n",
    "for i in range(1, last_movie+1):\n",
    "    if not i in df_table.columns:\n",
    "        df_table[i] = np.NaN\n",
    "df_table = df_table[[i for i in range(1, last_movie+1)]]\n",
    "print(df_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 256 1057    1]\n",
      " [ 903 1073    1]\n",
      " [ 427  990    1]\n",
      " ...\n",
      " [  85   53    1]\n",
      " [ 772  259    1]\n",
      " [ 429  366    1]]\n",
      "(76000, 2)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "pre_use_mat = df_table.replace([1.0, 2.0, 3.0, 4.0, 5.0], 1.0).values\n",
    "pre_use_val = copy.deepcopy(val_data.values)\n",
    "for temp in pre_use_val:\n",
    "    temp[2] = 1\n",
    "print(pre_use_val)\n",
    "row = pre_use_mat.shape[0]\n",
    "col = pre_use_mat.shape[1]\n",
    "rated_indexes = np.argwhere(pre_use_mat == 1)\n",
    "print(rated_indexes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_with_bias(y, p, q, b_p, b_q, mean):\n",
    "    return ((y-mean-b_p-b_q-np.dot(p, q))**2+_lambda*np.sqrt(np.sum(np.square(p))) + _lambda*np.sqrt(np.sum(np.square(q))) + _lambda*(b_p**2) + _lambda*(b_q**2) + 0.e-10)\n",
    "\n",
    "def mf_uninteresting(index_arr, validation_set):\n",
    "    p = np.random.rand(row, attr_num)  # number of parameter : row * 3\n",
    "    q = np.random.rand(col, attr_num)  # number of parameter : col * 3\n",
    "    b_p = np.random.rand(row, 1)\n",
    "    b_q = np.random.rand(col, 1)\n",
    "    min_rmse_error = 9999999\n",
    "    best_p = None\n",
    "    best_q = None\n",
    "    best_b_p = None\n",
    "    best_b_q = None\n",
    "    not_improved_cnt = 0\n",
    "    for _ in range(k):\n",
    "        d_p = [[np.zeros(attr_num), 0] for _ in range(row)]\n",
    "        d_q = [[np.zeros(attr_num), 0] for _ in range(col)]\n",
    "        d_b_p = [[0, 0] for _ in range(row)]\n",
    "        d_b_q = [[0, 0] for _ in range(col)]\n",
    "        error = 0\n",
    "        for i, j in index_arr:\n",
    "            p_i = p[i]\n",
    "            q_j = q[j]\n",
    "            b_p_i = b_p[i][0]\n",
    "            b_q_j = b_q[j][0]\n",
    "            a = np.dot(p_i, q_j)\n",
    "            # values of derivation p\n",
    "            d_p[i][0] += _lambda*p_i - (1-b_p_i-b_q_j-1-a)*q_j\n",
    "            d_p[i][1] += 1  # cnt\n",
    "            d_q[j][0] += _lambda*q_j - (1-b_p_i-b_q_j-1-a)*p_i\n",
    "            d_q[j][1] += 1\n",
    "            d_b_p[i][0] += _lambda*b_p_i - (1-b_p_i-b_q_j-1-a)\n",
    "            d_b_p[i][1] += 1\n",
    "            d_b_q[j][0] += _lambda*b_q_j - (1-b_p_i-b_q_j-1-a)\n",
    "            d_b_q[j][1] += 1\n",
    "            error += mean_squared_error_with_bias(\n",
    "                1, p[i], q[j], b_p_i, b_q_j, 1)\n",
    "            \n",
    "        # current value - derivation_averageÂ«\n",
    "        for i in range(row):\n",
    "            if d_p[i][1] != 0:\n",
    "                p[i] -= alpha*(d_p[i][0]/d_p[i][1])\n",
    "                b_p[i][0] -= alpha*(d_b_p[i][0]/d_b_p[i][1])\n",
    "        # current value - derivation_average\n",
    "        for j in range(col):\n",
    "            if d_q[j][1] != 0:\n",
    "                q[j] -= alpha*(d_q[j][0]/d_q[j][1])\n",
    "                b_q[j][0] -= alpha*(d_b_q[j][0]/d_b_q[j][1])\n",
    "        error_ = error/len(index_arr)\n",
    "        print(\"Training_error : \", error_)\n",
    "        result_temp = np.dot(p, q.transpose()) + 1\n",
    "        result_temp += b_p\n",
    "        result_temp += b_q.transpose()\n",
    "        rmse_error = 0\n",
    "        for user, movie, rating in validation_set:\n",
    "            rmse_error += (result_temp[user-1][movie-1]-rating) ** 2\n",
    "        rmse_error = np.sqrt(rmse_error/len(validation_set))\n",
    "        print(\"Validation Error : \", rmse_error)\n",
    "        if rmse_error < min_rmse_error:\n",
    "            min_rmse_error = rmse_error\n",
    "            best_p = p\n",
    "            best_q = q\n",
    "            best_b_p = b_p\n",
    "            best_b_q = b_q\n",
    "        else:\n",
    "            not_improved_cnt += 1\n",
    "            print(f\"Did not improved from {min_rmse_error} to {rmse_error}\")\n",
    "            if not_improved_cnt >= patience:\n",
    "                print(\"Early Stopped!!\")\n",
    "                return best_p, best_q, best_b_p, best_b_q\n",
    "    return best_p, best_q, best_b_p, best_b_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_error :  16.12760368669103\n",
      "Validation Error :  1.0731984741386136\n",
      "Training_error :  1.4507832489188375\n",
      "Validation Error :  0.5662620049819616\n",
      "Training_error :  0.5239191165466651\n",
      "Validation Error :  0.3469546119698188\n",
      "Training_error :  0.29445709066867537\n",
      "Validation Error :  0.2533273719784503\n",
      "Training_error :  0.22495152512051256\n",
      "Validation Error :  0.20931082265108472\n",
      "Training_error :  0.19617547407672387\n",
      "Validation Error :  0.18392341534652779\n",
      "Training_error :  0.17969446905473774\n",
      "Validation Error :  0.1662599491262555\n",
      "Training_error :  0.16795018814331203\n",
      "Validation Error :  0.1524845038342297\n",
      "Training_error :  0.15855689022682176\n",
      "Validation Error :  0.1410506757162418\n",
      "Training_error :  0.15058046327735494\n",
      "Validation Error :  0.13121014414669635\n",
      "Training_error :  0.14357370079318046\n",
      "Validation Error :  0.12254296073134607\n",
      "Training_error :  0.13728782517709584\n",
      "Validation Error :  0.11478946036915247\n",
      "Training_error :  0.1315690929605342\n",
      "Validation Error :  0.10777824920760216\n",
      "Training_error :  0.1263147474332311\n",
      "Validation Error :  0.10139017753477297\n",
      "Training_error :  0.12145168020840195\n",
      "Validation Error :  0.09553847520237119\n",
      "Training_error :  0.11692517267975429\n",
      "Validation Error :  0.0901572009607313\n",
      "Training_error :  0.11269258652781362\n",
      "Validation Error :  0.08519430959523386\n",
      "Training_error :  0.10871964531890138\n",
      "Validation Error :  0.08060738568827674\n",
      "Training_error :  0.10497813550708976\n",
      "Validation Error :  0.076360951195087\n",
      "Training_error :  0.10144441563512215\n",
      "Validation Error :  0.07242471726399902\n",
      "Training_error :  0.09809840372436056\n",
      "Validation Error :  0.0687724120994159\n",
      "Training_error :  0.09492285925111822\n",
      "Validation Error :  0.06538096713117815\n",
      "Training_error :  0.09190285440378536\n",
      "Validation Error :  0.06222993128369421\n",
      "Training_error :  0.08902537219134282\n",
      "Validation Error :  0.05930103444864006\n",
      "Training_error :  0.08627899303232475\n",
      "Validation Error :  0.056577851582931264\n",
      "Training_error :  0.08365364532114704\n",
      "Validation Error :  0.05404553693934114\n",
      "Training_error :  0.08114040370687976\n",
      "Validation Error :  0.05169060885427518\n",
      "Training_error :  0.07873132387858339\n",
      "Validation Error :  0.04950077220348116\n",
      "Training_error :  0.07641930586927405\n",
      "Validation Error :  0.047464769803092825\n",
      "Training_error :  0.07419798001225196\n",
      "Validation Error :  0.04557225668215185\n",
      "Training_error :  0.07206161113176426\n",
      "Validation Error :  0.04381369287388945\n",
      "Training_error :  0.07000501757073128\n",
      "Validation Error :  0.0421802515171373\n",
      "Training_error :  0.06802350239871624\n",
      "Validation Error :  0.04066373983662713\n",
      "Training_error :  0.06611279469354078\n",
      "Validation Error :  0.03925653110961588\n",
      "Training_error :  0.06426899920738134\n",
      "Validation Error :  0.03795150610547197\n",
      "Training_error :  0.062488553050372264\n",
      "Validation Error :  0.036742002754945366\n",
      "Training_error :  0.06076818827670371\n",
      "Validation Error :  0.035621772999840484\n",
      "Training_error :  0.05910489945769711\n",
      "Validation Error :  0.034584945914299496\n",
      "Training_error :  0.05749591548576199\n",
      "Validation Error :  0.033625996291994774\n",
      "Training_error :  0.055938674981480266\n",
      "Validation Error :  0.03273971797118724\n",
      "Training_error :  0.05443080478024172\n",
      "Validation Error :  0.03192120123091818\n",
      "Training_error :  0.052970101059823874\n",
      "Validation Error :  0.031165813643513157\n",
      "Training_error :  0.051554512740057026\n",
      "Validation Error :  0.030469183816216553\n",
      "Training_error :  0.05018212684314801\n",
      "Validation Error :  0.029827187501725604\n",
      "Training_error :  0.04885115555089369\n",
      "Validation Error :  0.029235935605804907\n",
      "Training_error :  0.04755992473455383\n",
      "Validation Error :  0.02869176367096834\n",
      "Training_error :  0.046306863766198775\n",
      "Validation Error :  0.028191222468321352\n",
      "Training_error :  0.04509049644803684\n",
      "Validation Error :  0.027731069384200155\n",
      "Training_error :  0.04390943291946331\n",
      "Validation Error :  0.027308260342868994\n",
      "Training_error :  0.042762362421226295\n",
      "Validation Error :  0.02691994205965024\n"
     ]
    }
   ],
   "source": [
    "uninteresting_p, uninteresting_q, uninteresting_b_p, uninteresting_b_q = mf_uninteresting(rated_indexes, pre_use_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00210631 1.00616653 1.00129839 ... 1.66204188 1.197763   1.92260687]\n",
      " [1.00866594 1.00514494 1.01342652 ... 1.5915376  0.9717467  1.71846034]\n",
      " [1.00481494 0.99889665 1.00365302 ... 1.65497047 1.13311294 1.882251  ]\n",
      " ...\n",
      " [1.02614765 1.00466332 0.97736307 ... 1.66018522 1.22441314 1.90206342]\n",
      " [1.01174425 0.99579565 1.0016664  ... 1.43271547 1.03422293 1.78971334]\n",
      " [0.9843336  0.98732435 0.98555258 ... 1.6461741  1.21607692 1.90282125]]\n",
      "(17, 2)\n"
     ]
    }
   ],
   "source": [
    "pre_use_mat_result = np.dot(uninteresting_p, uninteresting_q.transpose()) + 1\n",
    "pre_use_mat_result += uninteresting_b_p\n",
    "pre_use_mat_result += uninteresting_b_q.transpose() \n",
    "print(pre_use_mat_result)\n",
    "uninteresting_rating_index = np.argwhere(pre_use_mat_result < 0.8)\n",
    "print(uninteresting_rating_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n",
      "[[nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682], [nan, 1682]]\n"
     ]
    }
   ],
   "source": [
    "original_rating_mat = df_table.fillna(-1).values\n",
    "for index in uninteresting_rating_index:\n",
    "    original_rating_mat[index[0]][index[1]] = 1\n",
    "for_mean = []\n",
    "for row in original_rating_mat:\n",
    "    cnt = 0\n",
    "    row_sum = 0\n",
    "    for rating_ in row:\n",
    "        if rating_ != -1:\n",
    "            cnt += 1\n",
    "            row_sum += rating_\n",
    "    for_mean.append([row_sum, cnt])\n",
    "print(for_mean)\n",
    "result_rated_indexes = np.argwhere(original_rating_mat)\n",
    "rating_sum = 0\n",
    "cnt = 0\n",
    "for index in result_rated_indexes:\n",
    "    rating_sum += original_rating_mat[index[0]][index[1]]\n",
    "    cnt += 1\n",
    "rating_mean = rating_sum / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_with_bias(y, p, q, b_p, b_q, mean):\n",
    "    return ((y-mean-b_p-b_q-np.dot(p, q))**2+_lambda*np.sqrt(np.sum(np.square(p))) + _lambda*np.sqrt(np.sum(np.square(q))) + _lambda*(b_p**2) + _lambda*(b_q**2) + 0.e-10)\n",
    "\n",
    "\n",
    "def mf_training(index_arr, data_arr, validation_set, mean):\n",
    "    p = np.random.rand(row, attr_num)  # number of parameter : row * attr_num\n",
    "    q = np.random.rand(col, attr_num)  # number of parameter : col * attr_num\n",
    "    b_p = np.random.rand(row, 1)\n",
    "    b_q = np.random.rand(col, 1)\n",
    "    min_rmse_error = 9999999\n",
    "    best_p = None\n",
    "    best_q = None\n",
    "    best_b_p = None\n",
    "    best_b_q = None\n",
    "    not_improved_cnt = 0\n",
    "    for _ in range(k):\n",
    "        d_p = [[np.zeros(attr_num), 0] for _ in range(row)]\n",
    "        d_q = [[np.zeros(attr_num), 0] for _ in range(col)]\n",
    "        d_b_p = [[0, 0] for _ in range(row)]\n",
    "        d_b_q = [[0, 0] for _ in range(col)]\n",
    "        acc_error = 0\n",
    "        rmse_error = 0\n",
    "        for i, j in index_arr:\n",
    "            p_i = p[i]\n",
    "            q_j = q[j]\n",
    "            b_p_i = b_p[i][0]\n",
    "            b_q_j = b_q[j][0]\n",
    "            a = np.dot(p_i, q_j)\n",
    "            # values of derivation p\n",
    "            d_p[i][0] += _lambda*p_i - (data_arr[i][j]-b_p_i-b_q_j-mean-a)*q_j\n",
    "            d_p[i][1] += 1  # cnt\n",
    "            d_q[j][0] += _lambda*q_j - (data_arr[i][j]-b_p_i-b_q_j-mean-a)*p_i\n",
    "            d_q[j][1] += 1\n",
    "            d_b_p[i][0] += _lambda*b_p_i - (data_arr[i][j]-b_p_i-b_q_j-mean-a)\n",
    "            d_b_p[i][1] += 1\n",
    "            d_b_q[j][0] += _lambda*b_q_j - (data_arr[i][j]-b_p_i-b_q_j-mean-a)\n",
    "            d_b_q[j][1] += 1\n",
    "            acc_error += mean_squared_error_with_bias(\n",
    "                data_arr[i][j], p[i], q[j], b_p_i, b_q_j, mean)\n",
    "\n",
    "\n",
    "        # current value - derivation_averageÂ«\n",
    "        for i in range(row):\n",
    "            if d_p[i][1] != 0:\n",
    "                p[i] -= alpha*(d_p[i][0]/d_p[i][1])\n",
    "                b_p[i][0] -= alpha*(d_b_p[i][0]/d_b_p[i][1])\n",
    "        # current value - derivation_average\n",
    "        for j in range(col):\n",
    "            if d_q[j][1] != 0:\n",
    "                q[j] -= alpha*(d_q[j][0]/d_q[j][1])\n",
    "                b_q[j][0] -= alpha*(d_b_q[j][0]/d_b_q[j][1])\n",
    "        \n",
    "        acc_error_ = acc_error/len(index_arr)\n",
    "        print(\"Training Error : \", acc_error_)\n",
    "        # for validation\n",
    "        result_temp = np.dot(p, q.transpose())+mean\n",
    "        result_temp += b_p\n",
    "        result_temp += b_q.transpose()\n",
    "        for user, movie, rating in validation_set:\n",
    "            rmse_error += (result_temp[user-1][movie-1]-rating) ** 2\n",
    "        rmse_error = np.sqrt(rmse_error/len(validation_set))\n",
    "        print(\"Validation Error : \", rmse_error)\n",
    "        if rmse_error < min_rmse_error:\n",
    "            min_rmse_error = rmse_error\n",
    "            best_p = p\n",
    "            best_q = q\n",
    "            best_b_p = b_p\n",
    "            best_b_q = b_q\n",
    "        else:\n",
    "            not_improved_cnt += 1\n",
    "            print(f\"Did not improved from {min_rmse_error} to {rmse_error}\")\n",
    "            if not_improved_cnt >= patience:\n",
    "                print(\"Early Stopped!!\")\n",
    "                return best_p, best_q, best_b_p, best_b_q\n",
    "    return best_p, best_q, best_b_p, best_b_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = val_data.values\n",
    "print(valid_set)\n",
    "p, q, b_p, b_q = mf_training(result_rated_indexes, original_rating_mat, valid_set, rating_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_result = np.dot(p, q.transpose()) + rating_mean\n",
    "recommend_result += b_p\n",
    "recommend_result += b_q.transpose() \n",
    "recommend_result = np.where(recommend_result < 1.0, 1, recommend_result)\n",
    "recommend_result = np.where(recommend_result > 5.0, 5, recommend_result)\n",
    "print(recommend_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = 0\n",
    "cnt = 0\n",
    "for_write = []\n",
    "for user, movie, rating in testframe.values:\n",
    "    for_write.append([user, movie, recommend_result[user-1][movie-1]])\n",
    "    rmse += (rating-recommend_result[user-1][movie-1])**2\n",
    "rmse = np.sqrt(rmse/testframe.shape[0])\n",
    "print(rmse)\n",
    "df_write = pd.DataFrame(for_write)\n",
    "df_write.to_csv(train_file+\"_prediction.txt\", sep=\"\\t\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
